{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Selenium and BeautifulSoup to scrape Indeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Goal\" data-toc-modified-id=\"Goal-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Goal</a></span></li><li><span><a href=\"#Importing-relevant-libraries\" data-toc-modified-id=\"Importing-relevant-libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Importing relevant libraries</a></span></li><li><span><a href=\"#A-function-to-compile-a-dictionary-for-the-information-collected\" data-toc-modified-id=\"A-function-to-compile-a-dictionary-for-the-information-collected-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>A function to compile a dictionary for the information collected</a></span></li><li><span><a href=\"#Scrapping-the-web\" data-toc-modified-id=\"Scrapping-the-web-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Scrapping the web</a></span></li><li><span><a href=\"#Creating-the-new-dataframe\" data-toc-modified-id=\"Creating-the-new-dataframe-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Creating the new dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspecting-one-of-the-dataframes-created\" data-toc-modified-id=\"Inspecting-one-of-the-dataframes-created-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Inspecting one of the dataframes created</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "To scrape jobs off a job aggregation website (Indeed.com) to obtain information that can be used to firstly predict salary and also to determine industry skills that are relevant to specific roles and seniority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from scrapy.selector import Selector\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to compile a dictionary for the information collected\n",
    "I will be collecting 4 key bits of information from each job, the title, job description, salary and location. Once I have collected this data from a website, I will need it to be compiled into a format in which I can manipulate into a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information(title,description,salary,location):\n",
    "    info={'title':title,\n",
    "    'description':description,\n",
    "    'salary':salary,\n",
    "         'location':location}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping the web\n",
    "To scrape Indeed, I have chosen two tools (Selenium and BeautifulSoup). I will be using Selenium to act as my browser and navigate the website because there is quite a lot of information stored within the __java script__ that can only be scrapped if the java script runs.\n",
    "\n",
    "Once On the website I then pull the html and decipher the information I am after using __BeautifulSoup__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(url):\n",
    "    #Initiallising the driver\n",
    "    driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver.exe\")\n",
    "    \n",
    "    #Navigating to the base website with a predetermined search query in the URL\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    \n",
    "    #Use BeautifulSoup to pasrse the html into a format in which we can search for data\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #Find all the job titles on that page and store it in a list\n",
    "    list_links = driver.find_elements_by_class_name('jobtitle')\n",
    "    \n",
    "    #Create a new empty list and new empty dictionary to use for storing the information\n",
    "    newurl_list=[]\n",
    "    information_total={}\n",
    "    \n",
    "    # (i) will be the indexes for my final dictionary so I will instantiate it as (-1) to start\n",
    "    i=-1\n",
    "    \n",
    "    #Search the html to pull the total number of jobs found and \n",
    "    #use it to define how to move to the next page of job listings\n",
    "    #Once the pages have been established, close the driver\n",
    "    for number in soup.find_all('div', {'id':\"searchCount\"}):\n",
    "        number=number.text.split(' ')\n",
    "        total=number[-2]\n",
    "        print(total)\n",
    "        for i in range(0,int(total.replace(',', '')),10):\n",
    "            newurl=url+str(i)\n",
    "            newurl_list.append(newurl)\n",
    "    driver.close()\n",
    "    \n",
    "    #Open a new instance of the driver using the list of urls with the pages added to the urls\n",
    "    driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver.exe\")\n",
    "    for item in newurl_list:\n",
    "        driver.get(item)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "        list_links = driver.find_elements_by_class_name('jobtitle')\n",
    "        \n",
    "        #Start a try loop in case the for loop fails (pop-up windoes/redirects)\n",
    "        try:\n",
    "            \n",
    "            #For each job on the page, sleep for 1 second, this allows time for the whole page to laod\n",
    "            #i+1 becaue (i) is the key for that job,\n",
    "            #look for the job title, description, salary and location\n",
    "            for link in list_links:\n",
    "                i=i+1\n",
    "                link.click()\n",
    "                sleep(1)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "                for title in soup.find_all('div', {'id':\"vjs-jobtitle\"}):\n",
    "                    title=title.text\n",
    "                for description in soup.find_all('div', {'id':\"vjs-desc\"}):\n",
    "                    description=description.text\n",
    "                for salary in soup.find_all('div', {'id':\"vjs-jobinfo\"}):\n",
    "                    salary=salary.text\n",
    "                for location in soup.find_all('span', {'id':\n",
    "                    \"vjs-loc\"}):\n",
    "                    location=location.text\n",
    "                close=driver.find_element_by_id(\"vjs-x\")\n",
    "                close.click()\n",
    "                \n",
    "                #Use the function made previously to put the information into a dictionary with the unique key\n",
    "                information_total[i]=information(title,description,salary,location)\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    driver.close()\n",
    "    \n",
    "    #The result of running the function will be a dictionary of dictionaries which is what I am after\n",
    "    return information_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is as simple as running the function with the correct url and collecting the resulting dictionary and turning it into a dataframe which I can use for my analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "information_total=get_info(\"https://au.indeed.com/jobs?q=%22data+scientist%22&l=WA&start=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the new dataframe\n",
    "Dataframe is made using pandas but needs to be transposed due ot the layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(information_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting one of the dataframes created\n",
    "All the columns have been loaded with the right labels which is what I am after.\n",
    "\n",
    "This is just a small subset of the data I scrapped, I passed the function a total of 12 different queeries and I have 12 different csv files. This will mean I do have a lot of duplicates but I don't think this is an issue for me in the long term as I can easily drop any duplicates later on in the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>salary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data and Analytics Team | Perth\\n-------------...</td>\n",
       "      <td>- Perth WA</td>\n",
       "      <td>Data ScientistVGW - Perth WA</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Graduate Data Scientist-234026\\nWe believe suc...</td>\n",
       "      <td>- Perth WA</td>\n",
       "      <td>Graduate Data ScientistUGL Limited257 reviews ...</td>\n",
       "      <td>Graduate Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Requisition ID: 16575\\nJob Category: Consultin...</td>\n",
       "      <td>- Perth WA</td>\n",
       "      <td>Intermediate Data ScientistHatch279 reviews - ...</td>\n",
       "      <td>Intermediate Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Job ID: 554101\\n\\nJob type: Full Time - Fixed ...</td>\n",
       "      <td>- Perth WA</td>\n",
       "      <td>Lead Data ScientistDowner Group291 reviews - P...</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Permanent Role\\n\\nBuisness Critical Position\\n...</td>\n",
       "      <td>- Perth WA</td>\n",
       "      <td>Data ScientistMichael Page169 reviews - Perth ...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>About our Client:\\n\\nMy client is global leade...</td>\n",
       "      <td>- Perth WA</td>\n",
       "      <td>Data ScientistHydrogen Group6 reviews - Perth ...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Advanced Analytics Consulting Projects\\nPerman...</td>\n",
       "      <td>- Perth WA</td>\n",
       "      <td>Data ScientistKelly Services12,380 reviews - P...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Company:\\n\\nOur client is a well known pla...</td>\n",
       "      <td>- Perth WA</td>\n",
       "      <td>Data ScientistBeacham Group Pty Ltd - Perth WA...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Long-Term contract\\nSuperannuation paid on all...</td>\n",
       "      <td>- Perth WA</td>\n",
       "      <td>SAP Master Data ScientistChandler Macleod80 re...</td>\n",
       "      <td>SAP Master Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Newly created role for a Data Scientist who th...</td>\n",
       "      <td>- Western Australia</td>\n",
       "      <td>Applied Maths SpecialistTalent International6 ...</td>\n",
       "      <td>Applied Maths Specialist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          description              location  \\\n",
       "1   Data and Analytics Team | Perth\\n-------------...            - Perth WA   \n",
       "2   Graduate Data Scientist-234026\\nWe believe suc...            - Perth WA   \n",
       "3   Requisition ID: 16575\\nJob Category: Consultin...            - Perth WA   \n",
       "4   Job ID: 554101\\n\\nJob type: Full Time - Fixed ...            - Perth WA   \n",
       "5   Permanent Role\\n\\nBuisness Critical Position\\n...            - Perth WA   \n",
       "6   About our Client:\\n\\nMy client is global leade...            - Perth WA   \n",
       "7   Advanced Analytics Consulting Projects\\nPerman...            - Perth WA   \n",
       "8   The Company:\\n\\nOur client is a well known pla...            - Perth WA   \n",
       "9   Long-Term contract\\nSuperannuation paid on all...            - Perth WA   \n",
       "10  Newly created role for a Data Scientist who th...   - Western Australia   \n",
       "\n",
       "                                               salary  \\\n",
       "1                        Data ScientistVGW - Perth WA   \n",
       "2   Graduate Data ScientistUGL Limited257 reviews ...   \n",
       "3   Intermediate Data ScientistHatch279 reviews - ...   \n",
       "4   Lead Data ScientistDowner Group291 reviews - P...   \n",
       "5   Data ScientistMichael Page169 reviews - Perth ...   \n",
       "6   Data ScientistHydrogen Group6 reviews - Perth ...   \n",
       "7   Data ScientistKelly Services12,380 reviews - P...   \n",
       "8   Data ScientistBeacham Group Pty Ltd - Perth WA...   \n",
       "9   SAP Master Data ScientistChandler Macleod80 re...   \n",
       "10  Applied Maths SpecialistTalent International6 ...   \n",
       "\n",
       "                          title  \n",
       "1                Data Scientist  \n",
       "2       Graduate Data Scientist  \n",
       "3   Intermediate Data Scientist  \n",
       "4           Lead Data Scientist  \n",
       "5                Data Scientist  \n",
       "6                Data Scientist  \n",
       "7                Data Scientist  \n",
       "8                Data Scientist  \n",
       "9     SAP Master Data Scientist  \n",
       "10     Applied Maths Specialist  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_scientist_vic', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
