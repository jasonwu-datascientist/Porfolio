{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Salary from job description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Choosing-the-dependant-and-independant-variables\" data-toc-modified-id=\"Choosing-the-dependant-and-independant-variables-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Choosing the dependant and independant variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Independant-variables\" data-toc-modified-id=\"Independant-variables-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Independant variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Coding-dummy-variables\" data-toc-modified-id=\"Coding-dummy-variables-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Coding dummy variables</a></span></li></ul></li><li><span><a href=\"#Dependant-variable\" data-toc-modified-id=\"Dependant-variable-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Dependant variable</a></span></li><li><span><a href=\"#Splitting-to-train-and-test-data\" data-toc-modified-id=\"Splitting-to-train-and-test-data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Splitting to train and test data</a></span></li><li><span><a href=\"#Count-vectorizer\" data-toc-modified-id=\"Count-vectorizer-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Count vectorizer</a></span></li></ul></li><li><span><a href=\"#Fitting-the-first-iteration\" data-toc-modified-id=\"Fitting-the-first-iteration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Fitting the first iteration</a></span></li><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TFIDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lasso-regression-on-TFIDF\" data-toc-modified-id=\"Lasso-regression-on-TFIDF-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Lasso regression on TFIDF</a></span></li></ul></li><li><span><a href=\"#Random-forest-regressor\" data-toc-modified-id=\"Random-forest-regressor-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Random forest regressor</a></span></li><li><span><a href=\"#XGBoost-regression\" data-toc-modified-id=\"XGBoost-regression-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>XGBoost regression</a></span></li><li><span><a href=\"#What-if-I-took-it-as-a-classification?\" data-toc-modified-id=\"What-if-I-took-it-as-a-classification?-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>What if I took it as a classification?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-evaluation\" data-toc-modified-id=\"Model-evaluation-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Model evaluation</a></span></li><li><span><a href=\"#Model-interpretation\" data-toc-modified-id=\"Model-interpretation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Model interpretation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim for question 1 was to predict salaries for jobs based off the descriptions given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant libraries for this section\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV, Lasso\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.externals.six import StringIO  \n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pydotplus\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import operator\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the correct csv into pandas\n",
    "jobs=pd.read_csv('model_ready.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>revised_salary</th>\n",
       "      <th>rel_title</th>\n",
       "      <th>ranking</th>\n",
       "      <th>category</th>\n",
       "      <th>salary_level</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>cleaned_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Digital Sales Business Analyst\\r\\r\\r\\nOpportun...</td>\n",
       "      <td>Digital Business Analyst</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Business AnalystNormal</td>\n",
       "      <td>med</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Digital Sales Business Analyst\\r\\r\\r\\nOpportun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>About the role : As a Business Analyst you wou...</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Business AnalystNormal</td>\n",
       "      <td>med</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>About the role : As a Business Analyst you wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TNT Express is one of the world's leading prov...</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Business AnalystNormal</td>\n",
       "      <td>med</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Mascot</td>\n",
       "      <td>TNT Express is one of the world's leading prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For over 15 years Quantium have combined the b...</td>\n",
       "      <td>Agile Business Analyst</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Business AnalystNormal</td>\n",
       "      <td>med</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>For over 15 years Quantium have combined the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A vacancy exists in the above unit for a highl...</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Business AnalystNormal</td>\n",
       "      <td>med</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Darlinghurst</td>\n",
       "      <td>A vacancy exists in the above unit for a highl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Digital Sales Business Analyst\\r\\r\\r\\nOpportun...   \n",
       "1  About the role : As a Business Analyst you wou...   \n",
       "2  TNT Express is one of the world's leading prov...   \n",
       "3  For over 15 years Quantium have combined the b...   \n",
       "4  A vacancy exists in the above unit for a highl...   \n",
       "\n",
       "                      title  revised_salary         rel_title ranking  \\\n",
       "0  Digital Business Analyst        105000.0  Business Analyst  Normal   \n",
       "1          Business Analyst        102500.0  Business Analyst  Normal   \n",
       "2          Business Analyst        105000.0  Business Analyst  Normal   \n",
       "3    Agile Business Analyst        105000.0  Business Analyst  Normal   \n",
       "4          Business Analyst        105000.0  Business Analyst  Normal   \n",
       "\n",
       "                 category salary_level state          city  \\\n",
       "0  Business AnalystNormal          med   NSW        Sydney   \n",
       "1  Business AnalystNormal          med   NSW        Sydney   \n",
       "2  Business AnalystNormal          med   NSW        Mascot   \n",
       "3  Business AnalystNormal          med   NSW        Sydney   \n",
       "4  Business AnalystNormal          med   NSW  Darlinghurst   \n",
       "\n",
       "                                cleaned_descriptions  \n",
       "0  Digital Sales Business Analyst\\r\\r\\r\\nOpportun...  \n",
       "1  About the role : As a Business Analyst you wou...  \n",
       "2  TNT Express is one of the world's leading prov...  \n",
       "3  For over 15 years Quantium have combined the b...  \n",
       "4  A vacancy exists in the above unit for a highl...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick visual check of the data\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the dependant and independant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independant variables\n",
    "The independant variables I will be using to make my models will be the descriptions, states and cities. I did not chose to include relative titles or categories because I used that information to compute what the salaries should be so including them would artificailly improve my model due to their high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding dummy variables\n",
    "I used pandas to encode dummy variables for the cities and states for the jobs so I can use them for moddeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1315, 98)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummies = pd.get_dummies(jobs[['city','state']], drop_first=True)\n",
    "df_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dummy variables I encoded the only other variable to use as an independant variable is the job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I created a new dataframe concatenating the independant variables I have chosen\n",
    "X_df = pd.concat([df_dummies, jobs.cleaned_descriptions], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependant variable\n",
    "I decided to go with tackling the problem in terms of regression although it is not perfect and I anticipate won't yeild very good results. The reason I still decided to do this however is to see how well using text for regression analytics might fair as opposed the calssification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = jobs.revised_salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting to train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I split my data into a train and test set. I will be training all my models on the training set and testing on the test set. This will give me the best idea of how well generalized my model is. I opted with a training set size of 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, random_state=1,train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorizer\n",
    "I started my first model using count vectorizer with an n-gram range of 2-3 because I felt this would give me the best results in terms of correctly identifying important phrases. I also used stop words from the NLTK library and limited the max features to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,3),stop_words='english',max_features=100)\n",
    "X_train_dtm1 = vect.fit_transform(X_train.cleaned_descriptions)\n",
    "X_test_dtm1 = vect.transform(X_test.cleaned_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the result into a working state for my model, I had to transform it from a sparse matrix back to a dense one wit hthe proper headers denoting each phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count_vect_df = pd.DataFrame(X_train_dtm1.todense(), columns=vect.get_feature_names())\n",
    "test_count_vect_df = pd.DataFrame(X_test_dtm1.todense(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows me to then concatinate the vectorised description with the dummies I created earlier into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm=pd.concat([X_train.drop('cleaned_descriptions',axis=1).reset_index(drop=True), train_count_vect_df], axis=1)\n",
    "X_test_dtm=pd.concat([X_test.drop('cleaned_descriptions',axis=1).reset_index(drop=True), test_count_vect_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the first iteration\n",
    "\n",
    "I decided to fit the regression model using Lasso regression right off the bat as I believe it is necessary to reduce the number of independant variables which Lasso will do nicely. Using cross validation, I found the optimal alpha and then fit a model using it onto my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 473.605101764796\n",
      "Mean lasso CV R2: 0.19202148206025094\n"
     ]
    }
   ],
   "source": [
    "optimal_lasso = LassoCV(cv=4).fit(X_train_dtm,y_train)\n",
    "\n",
    "lasso = Lasso(alpha=optimal_lasso.alpha_)\n",
    "\n",
    "lasso_scores = cross_val_score(lasso,X_train_dtm,y_train, cv=4)\n",
    "print('Optimal alpha:',optimal_lasso.alpha_)\n",
    "print('Mean lasso CV R2:',np.mean(lasso_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso train score: 0.24311780530042626\n",
      "Lasso test score: 0.12028277998648207\n",
      "RMSE: 29753.824782576725\n"
     ]
    }
   ],
   "source": [
    "lasso.fit(X_train_dtm,y_train)\n",
    "y_pred=lasso.predict(X_test_dtm)\n",
    "print('Lasso train score:',lasso.score(X_train_dtm,y_train))\n",
    "print('Lasso test score:',lasso.score(X_test_dtm,y_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model created above does not perform particularly well for predicting the salary. This is evident with the low train and test R-squared scores. In terms of how wrong the predictions are, on average the predictions made with this model are off by __$29,753__. The main issue here is that I am taking this as a regression problem and assuming a linear relationship but the way I have imputed the salaries means it will not be strictly linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next task is to break down the job descriptions like I did during exploratory data analysis. From doing the EDA, it is clear the best results were from using TFIDF which gave a much more appropriate weightage to words for my dataset.I also decided to stem and lemmatize the descriptions to further improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I needed to create 2 functions from which I will both use to stem and lemmatize words\n",
    "def lemmatize_word(words):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    words = word_tokenize(words)\n",
    "    lemma_list=[]\n",
    "    for w in words:\n",
    "        lemma_list.append(lemma.lemmatize(w))\n",
    "    return (\" \".join(lemma_list))\n",
    "\n",
    "def stem_word(words):\n",
    "    stem=PorterStemmer()\n",
    "    words = word_tokenize(words)\n",
    "    stem_list=[]\n",
    "    for w in words:\n",
    "        stem_list.append(stem.stem(w))\n",
    "    return (\" \".join(stem_list))\n",
    "\n",
    "def stem_and_lemma(words):\n",
    "    new_article=lemmatize_word(words)\n",
    "    new_article=stem_word(words)\n",
    "    return new_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because now I am instead using a TFIDF vectorizer over a count vectorizer, I need to redo the steps above but simply change the vectorizer used to obtain my new independant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['stem_lem']=jobs.cleaned_descriptions.apply(lambda x : stem_and_lemma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.concat([df_dummies, jobs.stem_lem], axis=1)\n",
    "y = jobs.revised_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, random_state=1,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,3),stop_words='english',max_features=100)\n",
    "X_train_dtm1 = vect.fit_transform(X_train.stem_lem)\n",
    "X_test_dtm1 = vect.transform(X_test.stem_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count_vect_df = pd.DataFrame(X_train_dtm1.todense(), columns=vect.get_feature_names())\n",
    "test_count_vect_df = pd.DataFrame(X_test_dtm1.todense(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm=pd.concat([X_train.drop('stem_lem',axis=1).reset_index(drop=True), train_count_vect_df], axis=1)\n",
    "X_test_dtm=pd.concat([X_test.drop('stem_lem',axis=1).reset_index(drop=True), test_count_vect_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression on TFIDF\n",
    "I decided to use Lasso regression again here to really showcase the difference between the two vectorizers. Using TFIDF I definitely get a better R squared score but I also get a better RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 85.20377010304789\n",
      "Mean lasso CV R2: 0.2307490034997837\n"
     ]
    }
   ],
   "source": [
    "optimal_lasso = LassoCV(cv=4).fit(X_train_dtm,y_train)\n",
    "\n",
    "lasso = Lasso(alpha=optimal_lasso.alpha_)\n",
    "\n",
    "lasso_scores = cross_val_score(lasso,X_train_dtm,y_train, cv=4)\n",
    "print('Optimal alpha:',optimal_lasso.alpha_)\n",
    "print('Mean lasso CV R2:',np.mean(lasso_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso train score: 0.31332890180632267\n",
      "Lasso test score: 0.15479666738896813\n",
      "RMSE: 29164.320012346598\n"
     ]
    }
   ],
   "source": [
    "lasso.fit(X_train_dtm,y_train)\n",
    "y_pred=lasso.predict(X_test_dtm)\n",
    "print('Lasso train score:',lasso.score(X_train_dtm,y_train))\n",
    "print('Lasso test score:',lasso.score(X_test_dtm,y_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest regressor\n",
    "Using the TFIDF vectorizer, I fit a random forest regression on the data. A random forest model is basically made up of many decision trees, in this case I am using 1000. It then leverages their strong learning property to take the most popular prediction when the model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr=RandomForestRegressor(n_estimators=1000)\n",
    "rfr.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean random forest CV R2: 0.21928349557544752\n"
     ]
    }
   ],
   "source": [
    "rfr_scores = cross_val_score(rfr, X_train_dtm, y_train, cv=4)\n",
    "print('Mean random forest CV R2:',np.mean(rfr_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is a significant improvement on RMSE**\n",
    "\n",
    "This shows me that the random forest is much better at predicting the salaries given the variables I have chosen. If the focus was jsut on predicting salaries this would definitely take precedent over my Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest train score: 0.9032714218326503\n",
      "Random forest test score: 0.23701563063879216\n",
      "RMSE: 27709.52461573283\n"
     ]
    }
   ],
   "source": [
    "y_pred=rfr.predict(X_test_dtm)\n",
    "print('Random forest train score:',rfr.score(X_train_dtm,y_train))\n",
    "print('Random forest test score:',rfr.score(X_test_dtm,y_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost regression\n",
    "Seeing the promising results of ensemble modeling, I decided to fit another ensemble model. This time I went with XGBoost. While Random forest takes advantage of the strong learners that individual decision trees can be, XGBoost leverages their ability to be weak learners also. Using a series of weak learners and placing weightage on their mistakes every itteration the theory is we will have a much better prediction model.\n",
    "\n",
    "In this situation, it did not perform any better than the random forest model I used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost=XGBRegressor(n_estimators=1000)\n",
    "xgboost.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean random forest CV R2: 0.16320533570774864\n"
     ]
    }
   ],
   "source": [
    "xgboost_scores = cross_val_score(xgboost, X_train_dtm, y_train, cv=4)\n",
    "print('Mean random forest CV R2:',np.mean(xgboost_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest train score: 0.9710269244429689\n",
      "Random forest test score: 0.22555383634470572\n",
      "RMSE: 27916.87943475195\n"
     ]
    }
   ],
   "source": [
    "y_pred=xgboost.predict(X_test_dtm)\n",
    "print('Random forest train score:',xgboost.score(X_train_dtm,y_train))\n",
    "print('Random forest test score:',xgboost.score(X_test_dtm,y_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if I took it as a classification?\n",
    "I decided to fit a rudimentary model taking it as a classification problem which would fit the dataset much better. This is because of the way I imputed the salaries. Because there were so many NaN values for the salary, by imputing salaries like I did it essentially lent it self to being more  of a classification problem than a regression problem. The model I will fit is a random forest simply because it performed best when tackling the problem as a regression one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "med     0.647148\n",
       "low     0.180228\n",
       "high    0.172624\n",
       "Name: salary_level, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline = 64.7%\n",
    "y = jobs.salary_level\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L1 penatly logistic regression CV score: 0.8075602791563891\n"
     ]
    }
   ],
   "source": [
    "X_df = pd.concat([df_dummies, jobs.stem_lem], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, random_state=1,train_size=0.7)\n",
    "vect = TfidfVectorizer(ngram_range=(2,3),stop_words='english',max_features=100)\n",
    "X_train_dtm1 = vect.fit_transform(X_train.stem_lem)\n",
    "X_test_dtm1 = vect.transform(X_test.stem_lem)\n",
    "train_count_vect_df = pd.DataFrame(X_train_dtm1.todense(), columns=vect.get_feature_names())\n",
    "test_count_vect_df = pd.DataFrame(X_test_dtm1.todense(), columns=vect.get_feature_names())\n",
    "X_train_dtm=pd.concat([X_train.drop('stem_lem',axis=1).reset_index(drop=True), train_count_vect_df], axis=1)\n",
    "X_test_dtm=pd.concat([X_test.drop('stem_lem',axis=1).reset_index(drop=True), test_count_vect_df], axis=1)\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "classifier.fit(X_train_dtm, y_train)\n",
    "classifier_cross = cross_val_score(classifier, X_train_dtm, y_train, cv=4)\n",
    "print('Mean L1 penatly logistic regression CV score:',np.mean(classifier_cross));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "The model I fit performs better than the baseline which is extremely promising. It also seems to be quite consistent in predicting across all three bands of pay that I set out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       high       0.66      0.61      0.63        61\n",
      "        low       0.75      0.68      0.71        66\n",
      "        med       0.85      0.88      0.86       268\n",
      "\n",
      "avg / total       0.80      0.81      0.80       395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(X_test_dtm)\n",
    "classifier_v1=metrics.classification_report(y_test, y_pred)\n",
    "print(classifier_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model interpretation\n",
    "From the model it is clear that they are placing a lot of importance on a few key words that were actually used to define the salary level. In this case this is definitely not the goal for the classifier. On later itterations it is clear that I should infact add these titles to the stop words list so that they do not get used when doing the prediction and artifically improving the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: data analyst         Importance: 0.12\n",
      "Variable: senior busi          Importance: 0.11\n",
      "Variable: busi analyst         Importance: 0.06\n",
      "Variable: senior busi analyst  Importance: 0.04\n",
      "Variable: commun skill         Importance: 0.02\n",
      "Variable: data analysi         Importance: 0.02\n",
      "Variable: city_Sydney          Importance: 0.01\n",
      "Variable: state_NSW            Importance: 0.01\n",
      "Variable: state_VIC            Importance: 0.01\n",
      "Variable: abil work            Importance: 0.01\n",
      "Variable: analyst join         Importance: 0.01\n",
      "Variable: analyst work         Importance: 0.01\n",
      "Variable: appli thi            Importance: 0.01\n",
      "Variable: best practic         Importance: 0.01\n",
      "Variable: big data             Importance: 0.01\n",
      "Variable: busi analysi         Importance: 0.01\n",
      "Variable: busi need            Importance: 0.01\n",
      "Variable: busi process         Importance: 0.01\n",
      "Variable: busi requir          Importance: 0.01\n",
      "Variable: busi stakehold       Importance: 0.01\n",
      "Variable: busi unit            Importance: 0.01\n",
      "Variable: click appli          Importance: 0.01\n",
      "Variable: com au               Importance: 0.01\n",
      "Variable: comput scienc        Importance: 0.01\n",
      "Variable: data analyt          Importance: 0.01\n",
      "Variable: data scienc          Importance: 0.01\n",
      "Variable: data scientist       Importance: 0.01\n",
      "Variable: data set             Importance: 0.01\n",
      "Variable: data visualis        Importance: 0.01\n",
      "Variable: end end              Importance: 0.01\n",
      "Variable: excel commun         Importance: 0.01\n",
      "Variable: experi busi          Importance: 0.01\n",
      "Variable: experi data          Importance: 0.01\n",
      "Variable: experi work          Importance: 0.01\n",
      "Variable: experis com          Importance: 0.01\n",
      "Variable: financi servic       Importance: 0.01\n",
      "Variable: flexibl work         Importance: 0.01\n",
      "Variable: high level           Importance: 0.01\n",
      "Variable: highli desir         Importance: 0.01\n",
      "Variable: intern extern        Importance: 0.01\n",
      "Variable: machin learn         Importance: 0.01\n",
      "Variable: minimum year         Importance: 0.01\n",
      "Variable: month contract       Importance: 0.01\n",
      "Variable: opportun work        Importance: 0.01\n",
      "Variable: pleas appli          Importance: 0.01\n",
      "Variable: pleas contact        Importance: 0.01\n",
      "Variable: previou experi       Importance: 0.01\n",
      "Variable: problem solv         Importance: 0.01\n",
      "Variable: process improv       Importance: 0.01\n",
      "Variable: project manag        Importance: 0.01\n",
      "Variable: skill abil           Importance: 0.01\n",
      "Variable: skill experi         Importance: 0.01\n",
      "Variable: solv skill           Importance: 0.01\n",
      "Variable: stakehold manag      Importance: 0.01\n",
      "Variable: success applic       Importance: 0.01\n",
      "Variable: success thi          Importance: 0.01\n",
      "Variable: tertiari qualif      Importance: 0.01\n",
      "Variable: thi posit            Importance: 0.01\n",
      "Variable: thi role             Importance: 0.01\n",
      "Variable: verbal commun        Importance: 0.01\n",
      "Variable: work busi            Importance: 0.01\n",
      "Variable: work close           Importance: 0.01\n",
      "Variable: www experis com      Importance: 0.01\n",
      "Variable: year experi          Importance: 0.01\n",
      "Variable: city_Adelaide        Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(classifier.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X_test_dtm.columns, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:65]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "For this question as a regression problem I was not able to do very accurate predictions. The lowest RMSE I could acheive with my models was 27000 which means that on average my errors were rather large considering the salaries I used ranged between $19000 and $120000.\n",
    "\n",
    "Upon changing my approach to that of classification to better suit the data, I was able to obtain quite a decent preliminary model as seen with the f1 score. I can definitely see room for improvement however in this model because it is clear that I can better utilize the stopwords to get a much more generalized model that could be used in the real world where not all job descriptions will have a relevant title within the description potentially."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
